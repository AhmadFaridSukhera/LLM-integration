{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import openai\n",
    "import csv\n",
    "import ast\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    if url:\n",
    "        try:\n",
    "            r=requests.get(url)\n",
    "            if r.status_code==200:\n",
    "                soup=BeautifulSoup(r.text,\"lxml\")\n",
    "                text=[]\n",
    "                #<p>tags\n",
    "                p_tags=[]\n",
    "                max_tags = 20\n",
    "\n",
    "                names=soup.find_all(\"p\")\n",
    "                for i, tag in enumerate(names):\n",
    "                    if i >= max_tags:\n",
    "                        break  # Stop after extracting the first 7 tags\n",
    "                    if tag.text!='':\n",
    "                        name=tag.text\n",
    "                        p_tags.append(name)\n",
    "                text+=[str(item) for item in p_tags]\n",
    "                \n",
    "                #<a>tags\n",
    "                a_tags=[]\n",
    "                names=soup.find_all(\"a\")\n",
    "                for i, tag in enumerate(names):\n",
    "                    if i >= max_tags:\n",
    "                        break  # Stop after extracting the first 7 tags\n",
    "                    if tag.text!='':\n",
    "                        name=tag.text\n",
    "                        a_tags.append(name)\n",
    "                \n",
    "                text += [str(item) for item in a_tags]\n",
    "                #hrefs\n",
    "                hrefs = [a.get('href') for a in names]\n",
    "                social_media_keywords = [\n",
    "                \"facebook.com\",\n",
    "                \"linkedin.com\",\n",
    "                \"instagram.com\",\n",
    "                \"pinterest.com\",\n",
    "                \"snapchat.com\",\n",
    "                \"tiktok.com\",\n",
    "                \"reddit.com\",\n",
    "                ]\n",
    "                    # Initialize a list to store the selected social media links\n",
    "                selected_social_media_links = []\n",
    "\n",
    "                # Iterate through the hrefs and filter social media links\n",
    "                for href in hrefs:\n",
    "                    href=str(href)\n",
    "                    # Check if any of the social media keywords is present in the URL\n",
    "                    if any(keyword in href.lower() for keyword in social_media_keywords):\n",
    "                        selected_social_media_links.append(href)\n",
    "\n",
    "                # Now, 'selected_social_media_links' contains only the complete social media links\n",
    "\n",
    "                # Convert the selected social media links to a list of strings\n",
    "                text = [str(item) for item in selected_social_media_links]\n",
    "                #<h2>tags\n",
    "                h2_tags=[]\n",
    "                names=soup.find_all(\"h2\")\n",
    "                for i, tag in enumerate(names):\n",
    "                    if i >= max_tags:\n",
    "                        break  # Stop after extracting the first 7 tags\n",
    "                    if tag.text!='':\n",
    "                        name=tag.text\n",
    "                        h2_tags.append(name)\n",
    "                text+=[str(item) for item in h2_tags]\n",
    "                return text\n",
    "            else:\n",
    "                print(f\"Skipping {url} due to non-200 status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while connecting to {url}: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(input_string):\n",
    "    data_dict = ast.literal_eval(input_string)\n",
    "\n",
    "    # Define the desired CSV file path\n",
    "    csv_file_path = 'output.csv'\n",
    "\n",
    "    # Extract keys and values from the dictionary\n",
    "    keys = list(data_dict.keys())\n",
    "    values = list(data_dict.values())\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    csv_file_exists = False\n",
    "    try:\n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            if any(row for row in csv_reader):\n",
    "                csv_file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        \n",
    "        # Write header row only if the file is newly created\n",
    "        if not csv_file_exists:\n",
    "            csv_writer.writerow(keys)\n",
    "        \n",
    "        # Write data row with values\n",
    "        csv_writer.writerow(values)\n",
    "\n",
    "    print(f'Data has been appended to CSV file \"{csv_file_path}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai(file_path):\n",
    "    try:\n",
    "        openai.api_key = '",
    "        # Call the OpenAI API to generate scenario information from the prompt\n",
    "        # Read the contents of the file into a string\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "        response = openai.Completion.create(\n",
    "                engine=\"text-davinci-003\",\n",
    "                prompt=file_contents,\n",
    "                max_tokens=500,\n",
    "                stop=None,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            # Extract relevant information from the API response\\\n",
    "        extracted_response=response.choices[0].text\n",
    "        print(extracted_response)\n",
    "        return extracted_response\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling the OpenAI API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'url': 'https://www.synthesia.io/', 'short-description': 'Synthesia is a video creation tool for teams to create training videos at scale', 'key-features': 'Replace boring text, PowerPoints and PDFs with engaging videos, create high-quality sales enablement videos, transform help articles into short videos, create information security training videos, create professional-looking product explainers, create videos as easily as a slide deck, turn text into high-quality voiceovers with one click, create videos as diverse as your audience, keep video library up to date without reshoots, embed videos into favorite tools, 200+ free video templates, create AI videos', 'Use cases': 'Training, Sales, Help Articles, Information Security Training, Product Explainer Videos, Video Creation, Voiceover', 'Price model': 'Free', 'Social Links': 'https://www.linkedin.com/company/synthesia-technologies/', 'Categories': 'Video Creation, Voiceover'}\n",
      "Data has been appended to CSV file \"output.csv\".\n"
     ]
    }
   ],
   "source": [
    "file_path = \"URLs.txt\"\n",
    "# Read the contents of the file into an array (list)\n",
    "array_contents=[]\n",
    "with open(file_path, 'r') as file:\n",
    " for line in file:\n",
    "    # Append each line to the list\n",
    "    array_contents.append(line.strip())\n",
    "tool_info = {\n",
    "    \"url\": \"write url of the website here\",\n",
    "    \"short-description\": \"write a short description about the tool here\",\n",
    "    \"key-features\": \"Identify and extract the main features and functionalities the tool offers and write here\",\n",
    "    \"Use cases\": \"Recognize and list various scenarios or domains where the tool can be applied effectively here\",\n",
    "    \"Price model\": \"if no information available then write (free) here\",\n",
    "    \"Social Links\": \"Extract links to the tool's social media profiles (if present) and write here\",\n",
    "    \"Categories\": \"Identify and extract the categories or domains the tool falls under\"\n",
    "}\n",
    "\n",
    "file_path=\"nwes.txt\"\n",
    "for i in array_contents:\n",
    "    arr= [\"Read the following data taken from various tags from the html of webiste: \" ]\n",
    "    arr.append(i)\n",
    "    arr.append(scrape(i))\n",
    "    arr.append(\"--- give me an answer in the following format that i can add into a csv file with no more than 1000 characters: \")\n",
    "    arr.append(tool_info)\n",
    "    list_as_strings = [str(item) for item in arr]\n",
    "            # Write the list elements to the text file\n",
    "    with open(file_path, 'w') as file:\n",
    "            pass\n",
    "            for item in list_as_strings:\n",
    "                item_without_brackets_and_newlines = item.replace('[', '').replace(']', '').replace('\\n', '')\n",
    "                # Write each element to the file without newline characters\n",
    "                file.write(item_without_brackets_and_newlines + '\\n')\n",
    "\n",
    "    to_csv(ai(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
